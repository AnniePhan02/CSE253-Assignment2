{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Task 2 (NEW)\n",
        "\n",
        "## Overview\n",
        "\n",
        "We extended the functionality of the Piano Genie project by Google to generate music based on words typed with the full QWERTY keyboard. This allows us to also feed in and output velocity into the model, where the project did not do so before.\n",
        "\n",
        "The original Github for the project can be found here: https://github.com/chrisdonahue/piano-genie\n",
        "\n",
        "### Inspiration\n",
        "\n",
        "Monkeytype is a popular typing speed website that some of our members frequent. We were curious to see if we could generate music dynamically based on what the user was typing\n",
        "https://monkeytype.com/\n",
        "\n",
        "We saw the Piano Genie demo in class, and saw an opportunity to extend it to explore this idea. The original demo can be found here:\n",
        "https://www.i-am.ai/piano-genie.html\n",
        "\n",
        "\n",
        "## Usage\n",
        "\n",
        "Use our utility HTML script to open a window where you can type out a paragraph. The HTML script will encode your typing as a combination of keys clicked, timestamp, and your word per minute.\n",
        "\n",
        "\n",
        "The utility script will output these stats to a `.csv` file that can be passed into the model to generate new music.\n",
        "\n",
        "## QWERTY Input\n",
        "\n",
        "We experimented with a few different ideas for how the QWERTY keyboard could augment what music is played.\n",
        "\n",
        "### Velocity per row\n",
        "\n",
        "We tested assigning each row of a keyboard to different velocities to be encoded during typing. For instance, the row with the keys \"QWERTYUIOP\" would be assigned a higher velocity value than the row with the keys \"ZXCVBNM\".\n",
        "\n",
        "We kept 8 note bins (forcing keys past the 8th in the row counting left to right to represent the 8th bin) during our initial experiments.\n",
        "\n",
        "\n",
        "## Notes\n",
        "\n",
        "We left some of the original documentation in to assist with setting context and if the project reviewers are interested in what work was done before. All original documentation in pure markdown blocks have been surrounded by codeblocks, which can be removed for your viewing convenience. Additionally, inline markdown is most likely partially or fully from the original repository. We mark documentation written fully by us with the tag (NEW). Also credit to copilot for assistance with the project."
      ],
      "metadata": {
        "id": "nHExj3S6I7R4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Encoding (NEW)\n",
        "\n",
        "We created a new system in order to encode our user's musical performance on a QWERTY keyboard into a format interpretable by a machine learning model.\n",
        "\n",
        "### User UI\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/AnniePhan02/CSE253-Assignment2/main/task2/images/NewInputSystem.png\" width=600px/></center>\n",
        "\n",
        "The new UI for adding text is a simple text input box, with the ability to click in and type text. The user can then download their performance as a CSV file using the `Download CSV` button.\n",
        "\n",
        "### CSV File Format\n",
        "\n",
        "The first row of the CSV file is a header, containing the following column names:\n",
        "* Key: This column stores the key pressed by the user on the QWERTY keyboard.\n",
        "* Seconds: This column stores the time elapsed in seconds since the very first key press. The time is recorded with three decimal places.\n",
        "* WPM: This column stores the words per minute calculated at the moment the key was pressed. The WPM is recorded with one decimal place.\n",
        "\n",
        "Each subsequent row after the first in the CSV file represents a single key press event. The values in these rows correspond to the columns defined in the header: the specific key pressed, the time of the key press, and the calculated words per minute at that moment.\n",
        "\n",
        "For example, the row `I,3.496,10.3` indicates that the key \"I\" was pressed at 3.496 seconds after the initial key press, and at that time, the user's typing speed was 10.3 words per minute.\n",
        "\n",
        "### Example CSV of Short Performance\n",
        "\n",
        "In the below performance, the phrase `I am going to pass` was typed into the terminal.\n",
        "\n",
        "```\n",
        "Key,Seconds,WPM\n",
        "Shift,0.000,Infinity\n",
        "Shift,3.373,7.1\n",
        "I,3.496,10.3\n",
        " ,3.622,13.3\n",
        "w,3.697,16.2\n",
        "Backspace,3.957,18.2\n",
        "a,4.023,20.9\n",
        "m,4.121,23.3\n",
        " ,4.229,25.5\n",
        "g,4.298,27.9\n",
        "o,4.399,30.0\n",
        "i,4.538,31.7\n",
        "n,4.591,34.0\n",
        "g,4.665,36.0\n",
        " ,4.731,38.0\n",
        "t,4.798,40.0\n",
        "o,4.837,42.2\n",
        " ,4.923,43.9\n",
        "p,5.040,45.2\n",
        "a,5.102,47.0\n",
        "s,5.246,48.0\n",
        "s,5.367,49.2\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "7WAvX0j8stZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Script For User Input\n",
        "\n",
        "```HTML\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "  <body>\n",
        "    <textarea id=\"t\" rows=\"10\" cols=\"60\" placeholder=\"Start typing…\"></textarea>\n",
        "    <br />\n",
        "    <button id=\"download\">Download CSV</button>\n",
        "\n",
        "    <script>\n",
        "      let startTime = null;\n",
        "      let charCount = 0;\n",
        "      // Header: Key, Seconds since first press, WPM at that moment\n",
        "      const rows = [[\"Key\", \"Seconds\", \"WPM\"]];\n",
        "\n",
        "      const log = (key, secs, wpm) => {\n",
        "        rows.push([key, secs.toFixed(3), wpm.toFixed(1)]);\n",
        "      };\n",
        "\n",
        "      document.getElementById(\"t\").addEventListener(\"keydown\", (e) => {\n",
        "        if (startTime === null) {\n",
        "          startTime = performance.now();\n",
        "        }\n",
        "        charCount++;\n",
        "        const now = performance.now();\n",
        "        const elapsedMs = now - startTime;\n",
        "        const elapsedSecs = elapsedMs / 1000;\n",
        "        const elapsedMins = elapsedMs / 60000;\n",
        "        // WPM = (chars ÷ 5) ÷ elapsedMinutes\n",
        "        const wpm = charCount / 5 / elapsedMins;\n",
        "        log(e.key, elapsedSecs, wpm);\n",
        "      });\n",
        "\n",
        "      document.getElementById(\"download\").addEventListener(\"click\", () => {\n",
        "        const csvContent = rows.map((r) => r.join(\",\")).join(\"\\n\");\n",
        "        const blob = new Blob([csvContent], { type: \"text/csv\" });\n",
        "        const url = URL.createObjectURL(blob);\n",
        "        const a = document.createElement(\"a\");\n",
        "        a.href = url;\n",
        "        a.download = \"typing_wpm_timestamps.csv\";\n",
        "        document.body.appendChild(a);\n",
        "        a.click();\n",
        "        document.body.removeChild(a);\n",
        "        URL.revokeObjectURL(url);\n",
        "      });\n",
        "    </script>\n",
        "  </body>\n",
        "</html>\n",
        "```"
      ],
      "metadata": {
        "id": "mCBhlkkMsw6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial tests:\n",
        "\n",
        "During our initial tests on whether or not we could get the pipeline to run, we converted our performance into 8 bins using the following script:"
      ],
      "metadata": {
        "id": "WP_GYJ7iykX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 1\n",
        "def letter_to_button_keyboard(letter):\n",
        "    # Map letters on the keyboard to button indices, top row, middle row, bottom row\n",
        "    top = \"qwertyuiop\"\n",
        "    middle = \"asdfghjkl\"\n",
        "    bottom = \"zxcvbnm\"\n",
        "    if letter in top:\n",
        "        return min(top.index(letter), 8), 40\n",
        "    elif letter in middle:\n",
        "        return min(middle.index(letter), 8), 80\n",
        "    elif letter in bottom:\n",
        "        return min(bottom.index(letter), 8), 120\n",
        "    else:\n",
        "        return 0, 0"
      ],
      "metadata": {
        "id": "t2RWF2yAzRNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After working through the missing dependencies and various setup quirks, we were able to generate this with our input script and new music:"
      ],
      "metadata": {
        "id": "VoK4tT5DzSB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "@ STEVEN INSERT HERE"
      ],
      "metadata": {
        "id": "17jbauGHzfDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline (NEW):\n",
        "\n",
        "Below are our modifications to the original pipeline. The pipeline has been modified in order to use the following inputs and outputs:\n",
        "\n",
        "*   Input: our new CSV file format representing what keys users have clicked while typing\n",
        "*   Output: A MIDI file that converts general music \"bins\" (where the bins are the keys the users clicked) to proper notes. In other words, converts our user's keyboard input into a musical piece.\n",
        "\n",
        "We have modified the pipeline to accept **26 bins**, and also output **velocity**.\n",
        "\n"
      ],
      "metadata": {
        "id": "CkWvJHzirhtl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK41YT0yH3vs"
      },
      "source": [
        "<!-- # Music Co-creation Tutorial Part 1: Training a generative model of music\n",
        "### [Chris Donahue](https://chrisdonahue.com), [Anna Huang](https://research.google/people/105787/), [Jon Gillick](https://www.jongillick.com/)\n",
        "\n",
        "This is the first part of a two-part tutorial entitled [*Interactive music co-creation with PyTorch and TensorFlow.js*](https://github.com/chrisdonahue/music-cocreation-tutorial/), prepared as part of the ISMIR 2021 tutorial *Designing generative models for interactive co-creation*. This part of the tutorial will demonstrate how to **train a generative model of music in PyTorch**, and **port its weights to TensorFlow.js** format for interaction. The [final result is here](https://chrisdonahue.com/music-cocreation-tutorial)—see our [GitHub repo](https://github.com/chrisdonahue/music-cocreation-tutorial/) for part 2. -->\n",
        "\n",
        "\n",
        "```\n",
        "## Primer on Piano Genie\n",
        "\n",
        "The generative model we will train is called [Piano Genie](https://magenta.tensorflow.org/pianogenie) (Donahue et al. 2019). Piano Genie is a system which maps amateur improvisations on a miniature 8-button keyboard ([video](https://www.youtube.com/watch?v=YRb0XAnUpIk), [demo](https://piano-genie.glitch.me)) into realistic performances on a full 88-key piano.\n",
        "\n",
        "To achieve this, Piano Genie adopts an _autoencoder_ approach. First, an _encoder_ maps professional piano performances into this 8-button space. Then, a _decoder_ attempts to reconstruct the original piano performance from the 8-button version. The entire system is trained end-to-end to minimize the decoder's reconstruction error. At performance time, we replace the encoder with a user improvising on an 8-button controller, and use the pre-trained decoder to generate a corresponding piano performance.\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/chrisdonahue/music-cocreation-tutorial/main/part-1-py-training/figures/overview.png\" width=600px/></center>\n",
        "\n",
        "At a low-level, both the encoder and the decoder for Piano Genie are lightweight recurrent neural networks, which are suitable for real-time performance even on mobile CPUs. The discrete bottleneck is achieved using a technique called _integer-quantized autoencoding_ (IQAE), which was also proposed in the Piano Genie paper.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an946C19rSVJ"
      },
      "source": [
        "#@title **(Step 1)** Parse MIDI piano performances into simple lists of notes\n",
        "\n",
        "# @markdown *Note*: Check this box to rebuild the dataset from scratch.\n",
        "REBUILD_DATASET = False  # @param{type:\"boolean\"}\n",
        "\n",
        "# @markdown To train Piano Genie, we will use a dataset of professional piano performances called [MAESTRO](https://magenta.tensorflow.org/datasets/maestro) (Hawthorne et al. 2019).\n",
        "# @markdown Each performance in this dataset was captured by a Disklavier, a computerized piano which can record human performances in MIDI format, i.e., as timestamped sequences of notes.\n",
        "\n",
        "PIANO_LOWEST_KEY_MIDI_PITCH = 21\n",
        "PIANO_NUM_KEYS = 88\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def download_and_parse_maestro():\n",
        "    # Install pretty_midi\n",
        "    !!pip install pretty_midi\n",
        "    import pretty_midi\n",
        "\n",
        "    # Download MAESTRO dataset (Hawthorne+ 2018)\n",
        "    !!wget -nc https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip\n",
        "    !!unzip maestro-v2.0.0-midi.zip\n",
        "\n",
        "    # Parse MAESTRO dataset\n",
        "    dataset = defaultdict(list)\n",
        "    with open(\"maestro-v2.0.0/maestro-v2.0.0.json\", \"r\") as f:\n",
        "        for attrs in tqdm(json.load(f)):\n",
        "            split = attrs[\"split\"]\n",
        "            midi = pretty_midi.PrettyMIDI(\"maestro-v2.0.0/\" + attrs[\"midi_filename\"])\n",
        "            assert len(midi.instruments) == 1\n",
        "            # @markdown Formally, a piano performance is a sequence of notes: $\\mathbf{x} = (x_1, \\ldots, x_N)$, where each $x_i = (t_i, d_i, k_i, v_i)$, signifying:\n",
        "            notes = [\n",
        "                (\n",
        "                    # @markdown 1. (When the key was pressed) An _onset_ time $t_i \\in \\mathbb{T}$, where $\\mathbb{T} = \\{ t \\in \\mathbb{R} \\mid 0 \\leq t \\leq T \\}$\n",
        "                    float(n.start),\n",
        "                    # @markdown 2. (How long the key was held) A _duration_ $d_i \\in \\mathbb{R}_{>0}$\n",
        "                    float(n.end) - float(n.start),\n",
        "                    # @markdown 3. (Which key was pressed) A _key_ index $k_i \\in \\mathbb{K}$, where $\\mathbb{K} = \\{\\text{A0}, \\ldots, \\text{C8}\\}$ and $|\\mathbb{K}| = 88$\n",
        "                    int(n.pitch - PIANO_LOWEST_KEY_MIDI_PITCH),\n",
        "                    # @markdown 4. (How hard the key was pressed) A _velocity_ $v_i \\in \\mathbb{V}$, where $\\mathbb{V} = \\{1, \\ldots, 127\\}$\n",
        "                    int(n.velocity),\n",
        "                )\n",
        "                for n in midi.instruments[0].notes\n",
        "            ]\n",
        "\n",
        "            # This list is in sorted order of onset time, i.e., $t_{i-1} \\leq t_i ~\\forall~i \\in \\{2, \\ldots, N\\}$.\n",
        "            notes = sorted(notes, key=lambda n: (n[0], n[2]))\n",
        "            assert all(\n",
        "                [\n",
        "                    all(\n",
        "                        [\n",
        "                            # Start times should be non-negative\n",
        "                            n[0] >= 0,\n",
        "                            # Note durations should be strictly positive, i.e., $d_i > 0$\n",
        "                            n[1] > 0,\n",
        "                            # Key index should be in range of the piano\n",
        "                            0 <= n[2] and n[2] < PIANO_NUM_KEYS,\n",
        "                            # Velocity should be valid\n",
        "                            1 <= n[3] and n[3] < 128,\n",
        "                        ]\n",
        "                    )\n",
        "                    for n in notes\n",
        "                ]\n",
        "            )\n",
        "            dataset[split].append(notes)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "\n",
        "if REBUILD_DATASET:\n",
        "    DATASET = download_and_parse_maestro()\n",
        "    with gzip.open(\"maestro-v2.0.0-simple.json.gz\", \"w\") as f:\n",
        "        f.write(json.dumps(DATASET).encode(\"utf-8\"))\n",
        "else:\n",
        "    !!wget -nc https://github.com/chrisdonahue/music-cocreation-tutorial/raw/main/part-1-py-training/data/maestro-v2.0.0-simple.json.gz\n",
        "    with gzip.open(\"maestro-v2.0.0-simple.json.gz\", \"rb\") as f:\n",
        "        DATASET = json.load(f)\n",
        "\n",
        "print([(s, len(DATASET[s])) for s in [\"train\", \"validation\", \"test\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Inspect the data structure and content\n",
        "print(\"Dataset splits and number of performances:\")\n",
        "for split, performances in DATASET.items():\n",
        "    print(f\"- {split}: {len(performances)} performances\")\n",
        "\n",
        "# Inspect the first performance in the training set\n",
        "if DATASET['train']:\n",
        "    first_performance = DATASET['train'][0]\n",
        "    print(\"\\nStructure of the first performance (first 5 notes):\")\n",
        "    print(first_performance[:5])\n",
        "    print(\"\\nData types of the first note:\")\n",
        "    if first_performance:\n",
        "        first_note = first_performance[0]\n",
        "        for i, value in enumerate(first_note):\n",
        "            print(f\"- Element {i}: {type(value)}\")"
      ],
      "metadata": {
        "id": "exSv4-Hw9Phe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas matplotlib seaborn numpy"
      ],
      "metadata": {
        "id": "64Q6KGrE-BQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Step 2: Analyze note properties\n",
        "\n",
        "# Combine all notes from all splits into a single list for analysis\n",
        "all_notes = []\n",
        "for split, performances in DATASET.items():\n",
        "    for performance in performances:\n",
        "        all_notes.extend(performance)\n",
        "\n",
        "# Create a pandas DataFrame for easier analysis\n",
        "notes_df = pd.DataFrame(all_notes, columns=['onset_time', 'duration', 'key_index', 'velocity'])\n",
        "\n",
        "# Distribution of key indices\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(notes_df['key_index'], bins=PIANO_NUM_KEYS, kde=False)\n",
        "plt.title('Distribution of Piano Key Indices')\n",
        "plt.xlabel('Key Index (0-87)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Distribution of velocities\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(notes_df['velocity'], bins=127, kde=False)\n",
        "plt.title('Distribution of Velocities')\n",
        "plt.xlabel('Velocity (1-127)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Distribution of durations\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(notes_df['duration'], bins=50, kde=True)\n",
        "plt.title('Distribution of Note Durations')\n",
        "plt.xlabel('Duration (seconds)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlim(0, 5) # Limit x-axis for better visualization of common durations\n",
        "plt.show()\n",
        "\n",
        "# Distribution of inter-onset intervals\n",
        "# Calculate inter-onset intervals for each performance\n",
        "all_iois = []\n",
        "for split, performances in DATASET.items():\n",
        "    for performance in performances:\n",
        "        onsets = [n[0] for n in performance]\n",
        "        iois = np.diff(onsets)\n",
        "        all_iois.extend(iois)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(all_iois, bins=50, kde=True)\n",
        "plt.title('Distribution of Inter-Onset Intervals')\n",
        "plt.xlabel('Inter-Onset Interval (seconds)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlim(0, 2) # Limit x-axis for better visualization of common intervals\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r9wx4cZu9hB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Analyze performance properties\n",
        "\n",
        "# Number of notes per performance\n",
        "num_notes_per_performance = [len(p) for split, performances in DATASET.items() for p in performances]\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(num_notes_per_performance, bins=50, kde=True)\n",
        "plt.title('Distribution of Number of Notes per Performance')\n",
        "plt.xlabel('Number of Notes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Total duration of performances\n",
        "total_duration_per_performance = [p[-1][0] - p[0][0] if p else 0 for split, performances in DATASET.items() for p in performances]\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(total_duration_per_performance, bins=50, kde=True)\n",
        "plt.title('Distribution of Total Performance Duration')\n",
        "plt.xlabel('Duration (seconds)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VbnJNEpv9g28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reintroducing Velocity to the autoencoder (NEW)\n",
        "\n",
        "As noted in the original documentation, the original version of Piano Genie did not utilize velocity to make its predictions. Since we use the entire QWERTY keyboard for our project, we saw an opportunity to test if the model performance would differ by adding velocity to the autoencoder.\n",
        "\n",
        "In the `PianoDecoder` class, we needed to reintroduce the velocity parameter initially dropped from the inputs, as shown below:\n",
        "\n",
        "```python\n",
        "inputs = [\n",
        "            F.one_hot(k_m1, PIANO_NUM_KEYS + 1),\n",
        "            t.unsqueeze(dim=2),\n",
        "            b.unsqueeze(dim=2),\n",
        "            v.unsqueeze(dim=2),\n",
        "        ]\n",
        "```\n",
        "\n",
        "We also had to do a similar thing for the encoder, changing function signatures:\n",
        "\n",
        "```python\n",
        "def forward(self, k, t, v):\n",
        "        inputs = [\n",
        "            F.one_hot(k, PIANO_NUM_KEYS),\n",
        "            t.unsqueeze(dim=2),\n",
        "            v.unsqueeze(dim=2),\n",
        "        ]\n",
        "```\n",
        "\n",
        "### Location in original docs mentioning lack of velocity:\n",
        "\n",
        "```\n",
        "we anticipate that it will be frustrating for users if the model predicts dynamics on their behalf, so we remove velocity terms  𝐯 :\n",
        "```"
      ],
      "metadata": {
        "id": "6uH16C43-ZIh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UQ9PAvMCwd2"
      },
      "source": [
        "# @title **(Step 2)** Define Piano Genie autoencoder\n",
        "\n",
        "# @markdown Our intended interaction for Piano Genie is to have users perform on a miniature 8-button keyboard and automatically map each of their button presses to a key on a piano.\n",
        "# @markdown Similarly to our formalization of piano performances, we will formalize a \"button performance\" as a sequence of \"notes\", where piano keys $k_i$ are replaced with buttons $b_i$, and we remove velocity since our button controller is not velocity sensitive.\n",
        "# @markdown So a button performance $\\mathbf{c}$ is:\n",
        "\n",
        "# @markdown - $\\mathbf{c} = (c_1, \\ldots, c_N)$, where $c_i = (t_i, d_i, b_i \\in \\mathbb{B})$, i.e., (onsets, durations, buttons), and $\\mathbb{B} = \\{ \\color{#EE2B29}\\blacksquare, \\color{#ff9800}\\blacksquare, \\color{#ffff00}\\blacksquare, \\color{#c6ff00}\\blacksquare, \\color{#00e5ff}\\blacksquare, \\color{#2979ff}\\blacksquare, \\color{#651fff}\\blacksquare, \\color{#d500f9}\\blacksquare \\}$\n",
        "\n",
        "# @markdown And a corresponding piano performance is:\n",
        "\n",
        "# @markdown - $\\mathbf{x} = (x_1, \\ldots, x_N)$, where $x_i = (t_i, d_i, k_i, v_i)$, i.e., (onsets, durations, keys, velocities)\n",
        "\n",
        "# @markdown To map button performances into piano performances, we will train a generative model $P(\\mathbf{x} \\mid \\mathbf{c})$.\n",
        "# @markdown In practice, we will factorize this joint distribution over note sequences $\\mathbf{x}$ into the product of conditional probabilities of individual notes: $P(\\mathbf{x} \\mid \\mathbf{c}) = \\prod_{i=1}^{N} P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c})$.\n",
        "\n",
        "# @markdown Hence, our **overall goal is to learn** $P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c})$,\n",
        "# @markdown which we will **approximate by modeling**:\n",
        "\n",
        "# @markdown <center>$P(k_i \\mid \\mathbf{k}_{<i}, \\mathbf{t}_{\\leq i}, \\mathbf{b}_{\\leq i})$.</center>\n",
        "\n",
        "# @markdown We arrived at this approximation by working through constraints imposed by the interaction (details at the end).\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# @markdown #### **Decoder**\n",
        "\n",
        "# @markdown <center><img src=\"https://raw.githubusercontent.com/chrisdonahue/music-cocreation-tutorial/main/part-1-py-training/figures/decoder.png\" width=600px/></center>\n",
        "# @markdown <center><b>Piano Genie decoder processing $N=4$ notes</b></center>\n",
        "\n",
        "# @markdown The approximation $P(k_i \\mid \\mathbf{k}_{<i}, \\mathbf{t}_{\\leq i}, \\mathbf{b}_{\\leq i})$ constitutes the decoder of Piano Genie, which we will parameterize using an RNN.\n",
        "# @markdown This is the portion of the model that users will interact with.\n",
        "# @markdown To achieve our intended real-time interaction, we will compute and sample from this RNN at the instant the user presses a button, passing as input the key from the previous timestep, the current time, the button the user pressed, and a vector which summarizes the ongoing history.\n",
        "\n",
        "# @markdown Formally, the decoder is a function:\n",
        "# @markdown $D_{\\theta}: k_{i-1}, t_i, b_i, \\mathbf{h}_{i-1} \\mapsto \\mathbf{\\hat{k}}_i, \\mathbf{h}_i$, where:\n",
        "\n",
        "# @markdown - $k_0$ is a special start-of-sequence token $<\\text{S}>$\n",
        "\n",
        "# @markdown - $\\mathbf{h}_i$ is a vector summarizing timesteps $1, \\ldots, i$\n",
        "\n",
        "# @markdown - $\\mathbf{h}_0$ is some initial value (zeros) for that vector\n",
        "\n",
        "# @markdown - $\\mathbf{\\hat{k}}_i \\in \\mathbb{R}^{88}$ are the output logits for timestep $i$\n",
        "\n",
        "SOS = PIANO_NUM_KEYS\n",
        "\n",
        "class PianoGenieDecoder(nn.Module):\n",
        "    def __init__(self, rnn_dim=128, rnn_num_layers=2):\n",
        "        super().__init__()\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.rnn_num_layers = rnn_num_layers\n",
        "        #change this to 4\n",
        "        self.input = nn.Linear(PIANO_NUM_KEYS + 4, rnn_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            rnn_dim,\n",
        "            rnn_dim,\n",
        "            rnn_num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=False,\n",
        "        )\n",
        "        self.output = nn.Linear(rnn_dim, 88)\n",
        "\n",
        "    def init_hidden(self, batch_size, device=None):\n",
        "        h = torch.zeros(self.rnn_num_layers, batch_size, self.rnn_dim, device=device)\n",
        "        c = torch.zeros(self.rnn_num_layers, batch_size, self.rnn_dim, device=device)\n",
        "        return (h, c)\n",
        "\n",
        "    def forward(self, k, t, b, v, h_0=None):\n",
        "        # Prepend <S> token to shift k_i to k_{i-1}\n",
        "        k_m1 = torch.cat([torch.full_like(k[:, :1], SOS), k[:, :-1]], dim=1)\n",
        "\n",
        "        # Encode input\n",
        "        inputs = [\n",
        "            F.one_hot(k_m1, PIANO_NUM_KEYS + 1),\n",
        "            t.unsqueeze(dim=2),\n",
        "            b.unsqueeze(dim=2),\n",
        "            v.unsqueeze(dim=2),\n",
        "        ]\n",
        "        x = torch.cat(inputs, dim=2)\n",
        "\n",
        "        # Project encoded inputs\n",
        "        x = self.input(x)\n",
        "\n",
        "        # Run RNN\n",
        "        if h_0 is None:\n",
        "            h_0 = self.init_hidden(k.shape[0], device=k.device)\n",
        "        x, h_N = self.lstm(x, h_0)\n",
        "\n",
        "        # Compute logits\n",
        "        hat_k = self.output(x)\n",
        "\n",
        "        return hat_k, h_N\n",
        "\n",
        "\n",
        "# @markdown #### **Encoder**\n",
        "\n",
        "# @markdown <center><img src=\"https://i.imgur.com/P3bQFsC.png\" width=600px/></center>\n",
        "# @markdown <center><b>Piano Genie encoder processing $N=4$ notes</b></center>\n",
        "\n",
        "# @markdown Because we lack examples of human button performances, we use an encoder to automatically learn to map piano performances into synthetic button performances.\n",
        "# @markdown The encoder takes as input a sequence of keys and onset times and produces an equal-length sequence of buttons.\n",
        "# @markdown Formally, the encoder is a function: $E_{\\varphi} : \\mathbf{k}, \\mathbf{t} \\mapsto \\mathbf{b}$.\n",
        "\n",
        "# @markdown Note the conceptual difference between the decoder and the encoder: the decoder process one sequence item at a time, while the encoder maps an entire input sequence to an output sequence.\n",
        "# @markdown This is because the decoder (which we will use during inference) needs to process information as it becomes available in real time, whereas the encoder (which we only use during training) can observe the entire piano sequence before translating it into buttons.\n",
        "# @markdown Despite this conceptual difference, in practice the encoder is also an RNN (though a bidirectional one) under the hood.\n",
        "\n",
        "class PianoGenieEncoder(nn.Module):\n",
        "    def __init__(self, rnn_dim=128, rnn_num_layers=2):\n",
        "        super().__init__()\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.rnn_num_layers = rnn_num_layers\n",
        "        self.input = nn.Linear(PIANO_NUM_KEYS + 2, rnn_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            rnn_dim,\n",
        "            rnn_dim,\n",
        "            rnn_num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "        self.output = nn.Linear(rnn_dim * 2, 1)\n",
        "\n",
        "    def forward(self, k, t, v):\n",
        "        inputs = [\n",
        "            F.one_hot(k, PIANO_NUM_KEYS),\n",
        "            t.unsqueeze(dim=2),\n",
        "            v.unsqueeze(dim=2),\n",
        "        ]\n",
        "        x = self.input(torch.cat(inputs, dim=2))\n",
        "        # NOTE: PyTorch uses zeros automatically if h is None\n",
        "        x, _ = self.lstm(x, None)\n",
        "        x = self.output(x)\n",
        "        return x[:, :, 0]\n",
        "\n",
        "\n",
        "# @markdown #### **Quantizing encoder output to discrete buttons**\n",
        "\n",
        "# @markdown <center><img src=\"https://raw.githubusercontent.com/chrisdonahue/music-cocreation-tutorial/main/part-1-py-training/figures/quantization.png\" width=600px/></center>\n",
        "# @markdown <center><b>Quantizing continuous encoder output (grey line) to eight discrete values (colorful line segments)</b></center>\n",
        "\n",
        "# @markdown You may have noticed in the code that the encoder outputs a real-valued scalar (let's call it $e_i \\in \\mathbb{R}$) at each timestep, but our goal is to output one of eight discrete buttons, i.e., $b_i \\in \\mathbb{B}$.\n",
        "# @markdown To achieve this, we will quantize this real-valued scalar as the centroid of the nearest of eight bins between $[-1, 1]$ (see figure above):\n",
        "\n",
        "# @markdown <center>$b_i = 2 \\cdot \\frac{\\tilde{b}_i - 1}{B - 1} - 1$, where $\\tilde{b}_i = \\text{round} \\left( 1 + (B - 1) \\cdot \\min \\left( \\max \\left( \\frac{e_i  + 1}{2}, 0 \\right), 1 \\right) \\right)$</center>\n",
        "\n",
        "class IntegerQuantizer(nn.Module):\n",
        "    def __init__(self, num_bins):\n",
        "        super().__init__()\n",
        "        self.num_bins = num_bins\n",
        "\n",
        "    def real_to_discrete(self, x, eps=1e-6):\n",
        "        x = (x + 1) / 2\n",
        "        x = torch.clamp(x, 0, 1)\n",
        "        x *= self.num_bins - 1\n",
        "        x = (torch.round(x) + eps).long()\n",
        "        return x\n",
        "\n",
        "    def discrete_to_real(self, x):\n",
        "        x = x.float()\n",
        "        x /= self.num_bins - 1\n",
        "        x = (x * 2) - 1\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Quantize and compute delta (used for straight-through estimator)\n",
        "        with torch.no_grad():\n",
        "            x_disc = self.real_to_discrete(x)\n",
        "            x_quant = self.discrete_to_real(x_disc)\n",
        "            x_quant_delta = x_quant - x\n",
        "\n",
        "        # @markdown In the backwards pass, we will use the straight-through estimator (Bengio et al. 2013), i.e., pretend that this discretization did not happen when computing gradients.\n",
        "        # Quantize w/ straight-through estimator\n",
        "        x = x + x_quant_delta\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# @markdown #### **Defining the autoencoder**\n",
        "\n",
        "# @markdown Finally, the Piano Genie autoencoder is simply the composition of the encoder, quantizer, and decoder.\n",
        "\n",
        "class PianoGenieAutoencoder(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.enc = PianoGenieEncoder(\n",
        "            rnn_dim=cfg[\"model_rnn_dim\"],\n",
        "            rnn_num_layers=cfg[\"model_rnn_num_layers\"],\n",
        "        )\n",
        "        self.quant = IntegerQuantizer(cfg[\"num_buttons\"])\n",
        "        self.dec = PianoGenieDecoder(\n",
        "            rnn_dim=cfg[\"model_rnn_dim\"],\n",
        "            rnn_num_layers=cfg[\"model_rnn_num_layers\"],\n",
        "        )\n",
        "\n",
        "    def forward(self, k, t, v):\n",
        "        e = self.enc(k, t, v)\n",
        "        b = self.quant(e)\n",
        "        hat_k, _ = self.dec(k, t, b, v)\n",
        "        return hat_k, e\n",
        "\n",
        "\n",
        "# @markdown #### **Approximating $P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c})$**\n",
        "\n",
        "# @markdown This section walks through how we designed an approximation to $P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c})$ which would be appropriate for our intended interaction. You probably don't need to understand this, but some may find it helpful as an illustration of how to design a generative model around constraints imposed by interaction.\n",
        "\n",
        "# @markdown First, we expand the terms, treating the onsets $\\mathbf{t}$ and durations $\\mathbf{d}$ as part of the button performance $\\mathbf{c}$:\n",
        "\n",
        "# @markdown <center>$P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c}) = P(k_i, v_i \\mid \\mathbf{k}_{<i}, \\mathbf{v}_{<i}, \\mathbf{t}, \\mathbf{d}, \\mathbf{b})$</center>\n",
        "\n",
        "# @markdown Because we want this interaction to be real-time, we must remove any information that might not be available at time $t_i$ (the moment the user presses a button), which includes future onsets $\\mathbf{t}_{>i}$, future buttons $\\mathbf{b}_{>i}$, and all durations $\\mathbf{d}$, since notes can be held indefinitely:\n",
        "\n",
        "# @markdown <center>$\\approx P(k_i, v_i \\mid \\mathbf{k}_{<i}, \\mathbf{v}_{<i}, \\mathbf{t}_{\\leq i}, \\mathbf{b}_{\\leq i})$</center>\n",
        "\n",
        "# @markdown Finally, we anticipate that it will be frustrating for users if the model predicts dynamics on their behalf, so we remove velocity terms $\\mathbf{v}$:\n",
        "\n",
        "# @markdown <center>$\\approx P(k_i, \\mid \\mathbf{k}_{<i}, \\mathbf{t}_{\\leq i}, \\mathbf{b}_{\\leq i})$</center>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (STEP 3) Modifying training pipeline (NEW)\n",
        "\n",
        "After modifying the autoencoder, we needed to figure out a way in order to fix the function calls in the training pipeline so the encoder could properly receive the velocity values, and that we could properly interpret the new output.\n",
        "\n",
        "We needed to add a new `batch_v` parameter. `batch_v` is a list that stores the velocity values for each note in a minibatch of piano performances. `n[3]` represents the velocity value in a note, which can be derived by looking at how the `download_and_parse_maestro` function structures the notes list.\n",
        "\n",
        "In context, the changes described are below.\n",
        "\n",
        "```python\n",
        "        # Key features\n",
        "        batch_k.append([n[2] for n in subsample])\n",
        "        batch_v.append([n[3] for n in subsample])\n",
        "\n",
        "        # Onset features\n",
        "        # NOTE: For stability, we pass delta time to Piano Genie instead of time.\n",
        "        t = np.diff([n[0] for n in subsample])\n",
        "        t = np.concatenate([[1e8], t])\n",
        "        t = np.clip(t, 0, CFG[\"data_delta_time_max\"])\n",
        "        batch_t.append(t)\n",
        "\n",
        "    return (torch.tensor(batch_k).long(), torch.tensor(batch_t).float(), torch.tensor(batch_v).float())\n",
        "```"
      ],
      "metadata": {
        "id": "Xgk96hep_mrZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_fReq-uCfoy"
      },
      "source": [
        "'@title **(Step 3)** Train Piano Genie'\n",
        "\n",
        "# @markdown *Note*: Check this box to log training curves to [Weights & Biases](https://wandb.ai/) (which will prompt you to log in).\n",
        "USE_WANDB = False  # @param{type:\"boolean\"}\n",
        "\n",
        "# @markdown Now that we've defined the autoencoder, we need to train it.\n",
        "# @markdown We will train the entire autoencoder end-to-end to minimize the reconstruction loss of the decoder.\n",
        "\n",
        "# @markdown <center>$\\mathcal{L}_{\\text{recons}} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{CrossEntropy}(\\text{Softmax}(\\mathbf{\\hat{k}}_i), k_i)$</center>\n",
        "\n",
        "# @markdown This loss alone does not encourage the encoder to produce button sequences with any particular structure, so the behavior of the decoder will likely be fairly unpredictable at interaction time.\n",
        "# @markdown We think it might be intuitive to users if the decoder respected the _contour_ of their performance, i.e., if higher buttons produced higher notes and lower buttons produced lower notes.\n",
        "# @markdown Hence, we include a loss term which encourages the encoder to produces button sequences which align with the contour of the piano key sequences.\n",
        "\n",
        "# @markdown <center>$\\mathcal{L}_{\\text{contour}} = \\frac{1}{N - 1} \\sum_{i=2}^{N} \\max (0, 1 - (k_i - k_{i-1}) \\cdot (e_i - e_{i-1}))^2$</center>\n",
        "\n",
        "# @markdown Finally, we find empirically that the encoder often outputs values outside of the $[-1, 1]$ range used for discretization.\n",
        "# @markdown Hence, we add a loss term which explicitly encourages this behavior\n",
        "\n",
        "# @markdown <center>$\\mathcal{L}_{\\text{margin}} = \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, |e_i| - 1)^2$</center>\n",
        "\n",
        "# @markdown Thus, our final loss function is:\n",
        "# @markdown <center>$\\mathcal{L} = \\mathcal{L}_{\\text{recons}} + \\mathcal{L}_{\\text{contour}} + \\mathcal{L}_{\\text{margin}}$</center>\n",
        "\n",
        "\n",
        "CFG = {\n",
        "    \"seed\": 0,\n",
        "    # Number of buttons in interface\n",
        "    \"num_buttons\": 26,\n",
        "    # Onset delta times will be clipped to this maximum\n",
        "    \"data_delta_time_max\": 1.0,\n",
        "    # Max time stretch for data augmentation (+- 5%)\n",
        "    \"data_augment_time_stretch_max\": 0.05,\n",
        "    # Max transposition for data augmentation (+- tritone)\n",
        "    \"data_augment_transpose_max\": 6,\n",
        "    # RNN dimensionality\n",
        "    \"model_rnn_dim\": 128,\n",
        "    # RNN num layers\n",
        "    \"model_rnn_num_layers\": 2,\n",
        "    # Training hyperparameters\n",
        "    \"batch_size\": 32,\n",
        "    \"seq_len\": 128,\n",
        "    \"lr\": 3e-4,\n",
        "    \"loss_margin_multiplier\": 1.0,\n",
        "    \"loss_contour_multiplier\": 1.0,\n",
        "    \"summarize_frequency\": 128,\n",
        "    \"eval_frequency\": 128,\n",
        "    \"max_num_steps\": 50000\n",
        "}\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "if USE_WANDB:\n",
        "    try:\n",
        "        import wandb\n",
        "    except ModuleNotFoundError:\n",
        "        !!pip install wandb\n",
        "        import wandb\n",
        "\n",
        "# Init\n",
        "run_dir = pathlib.Path(\"piano_genie\")\n",
        "run_dir.mkdir(exist_ok=True)\n",
        "with open(pathlib.Path(run_dir, \"cfg.json\"), \"w\") as f:\n",
        "    f.write(json.dumps(CFG, indent=2))\n",
        "if USE_WANDB:\n",
        "    wandb.init(project=\"music-cocreation-tutorial\", config=CFG, reinit=True)\n",
        "\n",
        "# Set seed\n",
        "if CFG[\"seed\"] is not None:\n",
        "    random.seed(CFG[\"seed\"])\n",
        "    np.random.seed(CFG[\"seed\"])\n",
        "    torch.manual_seed(CFG[\"seed\"])\n",
        "    torch.cuda.manual_seed_all(CFG[\"seed\"])\n",
        "\n",
        "# Create model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PianoGenieAutoencoder(CFG)\n",
        "model.train()\n",
        "model.to(device)\n",
        "print(\"-\" * 80)\n",
        "for n, p in model.named_parameters():\n",
        "    print(f\"{n}, {p.shape}\")\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=CFG[\"lr\"])\n",
        "\n",
        "# Subsamples performances to create a minibatch\n",
        "def performances_to_batch(performances, device, train=True):\n",
        "    batch_k = []\n",
        "    batch_t = []\n",
        "    batch_v = []\n",
        "    for p in performances:\n",
        "        # Subsample seq_len notes from performance\n",
        "        assert len(p) >= CFG[\"seq_len\"]\n",
        "        if train:\n",
        "            subsample_offset = random.randrange(0, len(p) - CFG[\"seq_len\"])\n",
        "        else:\n",
        "            subsample_offset = 0\n",
        "        subsample = p[subsample_offset : subsample_offset + CFG[\"seq_len\"]]\n",
        "        assert len(subsample) == CFG[\"seq_len\"]\n",
        "\n",
        "        # Data augmentation\n",
        "        if train:\n",
        "            stretch_factor = random.random() * CFG[\"data_augment_time_stretch_max\"] * 2\n",
        "            stretch_factor += 1 - CFG[\"data_augment_time_stretch_max\"]\n",
        "            transposition_factor = random.randint(\n",
        "                -CFG[\"data_augment_transpose_max\"], CFG[\"data_augment_transpose_max\"]\n",
        "            )\n",
        "            subsample = [\n",
        "                (\n",
        "                    n[0] * stretch_factor,\n",
        "                    n[1] * stretch_factor,\n",
        "                    max(0, min(n[2] + transposition_factor, PIANO_NUM_KEYS - 1)),\n",
        "                    n[3],\n",
        "                )\n",
        "                for n in subsample\n",
        "            ]\n",
        "\n",
        "        # Key features\n",
        "        batch_k.append([n[2] for n in subsample])\n",
        "        batch_v.append([n[3] for n in subsample])\n",
        "\n",
        "        # Onset features\n",
        "        # NOTE: For stability, we pass delta time to Piano Genie instead of time.\n",
        "        t = np.diff([n[0] for n in subsample])\n",
        "        t = np.concatenate([[1e8], t])\n",
        "        t = np.clip(t, 0, CFG[\"data_delta_time_max\"])\n",
        "        batch_t.append(t)\n",
        "\n",
        "    return (torch.tensor(batch_k).long(), torch.tensor(batch_t).float(), torch.tensor(batch_v).float())\n",
        "\n",
        "\n",
        "# Train\n",
        "step = 0\n",
        "best_eval_loss = float(\"inf\")\n",
        "while CFG[\"max_num_steps\"] is None or step < CFG[\"max_num_steps\"]:\n",
        "    if step % CFG[\"eval_frequency\"] == 0:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            eval_losses_recons = []\n",
        "            eval_violates_contour = []\n",
        "            for i in range(0, len(DATASET[\"validation\"]), CFG[\"batch_size\"]):\n",
        "                eval_batch = performances_to_batch(\n",
        "                    DATASET[\"validation\"][i : i + CFG[\"batch_size\"]],\n",
        "                    device,\n",
        "                    train=False,\n",
        "                )\n",
        "                eval_k, eval_t, eval_v = tuple(t.to(device) for t in eval_batch)\n",
        "                eval_hat_k, eval_e = model(eval_k, eval_t, eval_v)\n",
        "                eval_b = model.quant.real_to_discrete(eval_e)\n",
        "                eval_loss_recons = F.cross_entropy(\n",
        "                    eval_hat_k.view(-1, PIANO_NUM_KEYS),\n",
        "                    eval_k.view(-1),\n",
        "                    reduction=\"none\",\n",
        "                )\n",
        "                eval_violates = torch.logical_not(\n",
        "                    torch.sign(torch.diff(eval_k, dim=1))\n",
        "                    == torch.sign(torch.diff(eval_b, dim=1)),\n",
        "                ).float()\n",
        "                eval_violates_contour.extend(eval_violates.cpu().numpy().tolist())\n",
        "                eval_losses_recons.extend(eval_loss_recons.cpu().numpy().tolist())\n",
        "\n",
        "            eval_loss_recons = np.mean(eval_losses_recons)\n",
        "            if eval_loss_recons < best_eval_loss:\n",
        "                torch.save(model.state_dict(), pathlib.Path(run_dir, \"model.pt\"))\n",
        "                best_eval_loss = eval_loss_recons\n",
        "\n",
        "        eval_metrics = {\n",
        "            \"eval_loss_recons\": eval_loss_recons,\n",
        "            \"eval_contour_violation_ratio\": np.mean(eval_violates_contour),\n",
        "        }\n",
        "        if USE_WANDB:\n",
        "            wandb.log(eval_metrics, step=step)\n",
        "        print(step, \"eval\", eval_metrics)\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    # Create minibatch\n",
        "    batch = performances_to_batch(\n",
        "        random.sample(DATASET[\"train\"], CFG[\"batch_size\"]), device, train=True\n",
        "    )\n",
        "    k, t, v = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Run model\n",
        "    optimizer.zero_grad()\n",
        "    k_hat, e = model(k, t, v)\n",
        "\n",
        "    # Compute losses and update params\n",
        "    loss_recons = F.cross_entropy(k_hat.view(-1, PIANO_NUM_KEYS), k.view(-1))\n",
        "    loss_margin = torch.square(\n",
        "        torch.maximum(torch.abs(e) - 1, torch.zeros_like(e))\n",
        "    ).mean()\n",
        "    loss_contour = torch.square(\n",
        "        torch.maximum(\n",
        "            1 - torch.diff(k, dim=1) * torch.diff(e, dim=1),\n",
        "            torch.zeros_like(e[:, 1:]),\n",
        "        )\n",
        "    ).mean()\n",
        "    loss = torch.zeros_like(loss_recons)\n",
        "    loss += loss_recons\n",
        "    if CFG[\"loss_margin_multiplier\"] > 0:\n",
        "        loss += CFG[\"loss_margin_multiplier\"] * loss_margin\n",
        "    if CFG[\"loss_contour_multiplier\"] > 0:\n",
        "        loss += CFG[\"loss_contour_multiplier\"] * loss_contour\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "\n",
        "    if step % CFG[\"summarize_frequency\"] == 0:\n",
        "        metrics = {\n",
        "            \"loss_recons\": loss_recons.item(),\n",
        "            \"loss_margin\": loss_margin.item(),\n",
        "            \"loss_contour\": loss_contour.item(),\n",
        "            \"loss\": loss.item(),\n",
        "        }\n",
        "        if USE_WANDB:\n",
        "            wandb.log(metrics, step=step)\n",
        "        print(step, \"train\", metrics)\n",
        "\n",
        "# Download the trained model so we don't lose it!\n",
        "from google.colab import files\n",
        "\n",
        "files.download('piano_genie/model.pt')\n",
        "files.download('piano_genie/cfg.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ignoring Step 4 and 5 (NEW)\n",
        "\n",
        "We chose to ignore Step 4 and 5, as the old interface would not be compatible with a model that expects 26 bins.\n",
        "\n",
        "After Step 5, we will describe our new pipeline for encoding what the user types and interpreting that output file.\n"
      ],
      "metadata": {
        "id": "g3PEdlB9VxUo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj49WCPSAlyf"
      },
      "source": [
        "# # @title **(Step 4)** Port trained decoder parameters to Tensorflow.js format\n",
        "\n",
        "# # @markdown In this step, we will use the TensorFlow.js Python library to export our model's parameters in a binary format, to be loaded later by the JavaScript client.\n",
        "\n",
        "# !!pip install tensorflowjs\n",
        "\n",
        "# from tensorflowjs.write_weights import write_weights\n",
        "\n",
        "# # Load saved model dict\n",
        "# d = torch.load(\"piano_genie/model.pt\", map_location=torch.device(\"cpu\"))\n",
        "# d = {k: v.numpy() for k, v in d.items()}\n",
        "\n",
        "# # Convert to tensorflow-js format\n",
        "# pathlib.Path(\"piano_genie/dec_tfjs\").mkdir(exist_ok=True)\n",
        "# write_weights(\n",
        "#     [[{\"name\": k, \"data\": v} for k, v in d.items() if k.startswith(\"dec\")]],\n",
        "#     \"piano_genie/dec_tfjs\",\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBuHu2Hohc5f"
      },
      "source": [
        "# # @title **(Step 5)** Create test case to check correctness of JavaScript port\n",
        "\n",
        "# # @markdown Finally, we will serialize a sequence of inputs to and outputs from our trained model to create a test case for our JavaScript reimplementation.\n",
        "# # @markdown This is critically important—I have ported many models from Python to JavaScript and have yet to get it right on the first try.\n",
        "# # @markdown Porting models from PyTorch to TensorFlow.js is additionally tricky because parameters of the same shape are often used differently by the two APIs.\n",
        "\n",
        "# # Restore model from saved checkpoint\n",
        "# device = torch.device(\"cpu\")\n",
        "# with open(\"piano_genie/cfg.json\", \"r\") as f:\n",
        "#     cfg = json.load(f)\n",
        "# model = PianoGenieAutoencoder(cfg)\n",
        "# model.load_state_dict(torch.load(\"piano_genie/model.pt\", map_location=device))\n",
        "# model.eval()\n",
        "# model.to(device)\n",
        "\n",
        "# # Serialize a batch of inputs/outputs as JSON\n",
        "# with torch.no_grad():\n",
        "#     ground_truth_keys, input_dts = performances_to_batch(\n",
        "#         [DATASET[\"validation\"][0]], device, train=False\n",
        "#     )\n",
        "#     output_logits, input_buttons = model(ground_truth_keys, input_dts)\n",
        "#     input_buttons = model.quant.real_to_discrete(input_buttons)\n",
        "\n",
        "#     input_dts = input_dts[0].cpu().numpy().tolist()\n",
        "#     ground_truth_keys = ground_truth_keys[0].cpu().numpy().tolist()\n",
        "#     input_keys = [PIANO_NUM_KEYS] + ground_truth_keys[:-1]\n",
        "#     input_buttons = input_buttons[0].cpu().numpy().tolist()\n",
        "#     output_logits = output_logits[0].cpu().numpy().tolist()\n",
        "\n",
        "#     test = {\n",
        "#         n: eval(n)\n",
        "#         for n in [\"input_dts\", \"input_keys\", \"input_buttons\", \"output_logits\"]\n",
        "#     }\n",
        "#     with open(pathlib.Path(\"piano_genie\", \"test.json\"), \"w\") as f:\n",
        "#         f.write(json.dumps(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Step 4 and 5 (NEW)\n",
        "\n",
        "### Taking User Input\n",
        "\n",
        "We created a new HTML website that encodes user input as a CSV file. The file can be found on our Github here: https://github.com/AnniePhan02/CSE253-Assignment2/blob/main/task2/index.html\n",
        "\n",
        "### Feeding user input file into the model\n",
        "\n",
        "Running Steps 1-5 will result in two files: `cfg.json` and `model.pt`.\n",
        "\n",
        "The key function to generate output from the model is the `step()` function.\n",
        "\n"
      ],
      "metadata": {
        "id": "R3g-kjwrXwSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example `CFG.json`\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"seed\": 0,\n",
        "  \"num_buttons\": 26,\n",
        "  \"data_delta_time_max\": 1.0,\n",
        "  \"data_augment_time_stretch_max\": 0.05,\n",
        "  \"data_augment_transpose_max\": 6,\n",
        "  \"model_rnn_dim\": 128,\n",
        "  \"model_rnn_num_layers\": 2,\n",
        "  \"batch_size\": 32,\n",
        "  \"seq_len\": 128,\n",
        "  \"lr\": 0.0003,\n",
        "  \"loss_margin_multiplier\": 1.0,\n",
        "  \"loss_contour_multiplier\": 1.0,\n",
        "  \"summarize_frequency\": 128,\n",
        "  \"eval_frequency\": 128,\n",
        "  \"max_num_steps\": 50000\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "T3C76JmoyPKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the Newly Generated Model\n",
        "\n",
        "The repository we pulled from had some base code for training the model, but not for running it. We created the following script with the assistance of copilot in order to feed input and interpret output from the model."
      ],
      "metadata": {
        "id": "aBU1RAZRz_vV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Bins Based On Keys\n",
        "\n",
        "Since we have 26 keys, we made 26 bins\n",
        "\n",
        "```python\n",
        "def letter_to_button_26(letter):\n",
        "    # Map letters to button indices for a 26-letter keyboard layout\n",
        "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "    if letter in alphabet:\n",
        "        return alphabet.index(letter)\n",
        "    else:\n",
        "        return 0  # Default case for unsupported characters\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "XfrM5GL10jPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Script"
      ],
      "metadata": {
        "id": "644miOn_0XPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from genie import PianoGenieAutoencoder, SOS\n",
        "\n",
        "\n",
        "cfg = json.load(open(\"cfg.json\"))\n",
        "model = PianoGenieAutoencoder(cfg)\n",
        "model.load_state_dict(torch.load(\"model.pt\", map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# 2) Prepare decoder state for a single stream\n",
        "#    We’ll run batch_size=1 since we’re in interactive mode\n",
        "h = model.dec.init_hidden(batch_size=1)\n",
        "# k_prev holds the last output key index; start with SOS\n",
        "k_prev = torch.full((1, 1), SOS, dtype=torch.long)\n",
        "\n",
        "# 3) Now, each time the user presses a button, you have:\n",
        "#    b_i: integer 0…7    (which button)\n",
        "#    t_i: float          (absolute onset time in seconds)\n",
        "#    v_i: int 1…127      (velocity)\n",
        "#\n",
        "# You’ll convert these into tensors, call the decoder, sample/argmax,\n",
        "# and then feed that key back in as the next k_prev.\n",
        "\n",
        "\n",
        "def step(b_i, t_i, v_i, k_prev, h):\n",
        "    # 3a) button needs to be the *real-valued* centroid in [–1,1]\n",
        "    b_real = model.quant.discrete_to_real(torch.tensor([[b_i]]))  # → shape (1,1)\n",
        "    # 3b) wrap time & velocity\n",
        "    t = torch.tensor([[t_i]], dtype=torch.float)\n",
        "    v = torch.tensor([[v_i]], dtype=torch.float)\n",
        "\n",
        "    # 3c) run decoder for one timestep\n",
        "    with torch.no_grad():\n",
        "        logits, h = model.dec(k_prev, t, b_real, v, h)\n",
        "        # logits: (1,1,88)\n",
        "        probs = torch.softmax(logits[0, 0], dim=-1)\n",
        "        k_i = torch.multinomial(probs, num_samples=1)  # or .argmax()\n",
        "\n",
        "    return k_i.reshape(1, 1), h, probs\n",
        "\n",
        "\n",
        "# 4) Example usage:\n",
        "#    Suppose the user hits button 3 at time=0.57s with velocity=90:\n",
        "# k1, h, p = step(b_i=3, t_i=0.57, v_i=90, k_prev=k_prev, h=h)\n",
        "\n",
        "\n",
        "# version 1\n",
        "def letter_to_button_keyboard(letter):\n",
        "    # Map letters on the keyboard to button indices, top row, middle row, bottom row\n",
        "    top = \"qwertyuiop\"\n",
        "    middle = \"asdfghjkl\"\n",
        "    bottom = \"zxcvbnm\"\n",
        "    if letter in top:\n",
        "        return min(top.index(letter), 8), 40\n",
        "    elif letter in middle:\n",
        "        return min(middle.index(letter), 8), 80\n",
        "    elif letter in bottom:\n",
        "        return min(bottom.index(letter), 8), 120\n",
        "    else:\n",
        "        return 0, 0\n",
        "\n",
        "\n",
        "def letter_to_button_26(letter):\n",
        "    # Map letters to button indices for a 26-letter keyboard layout\n",
        "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "    if letter in alphabet:\n",
        "        return alphabet.index(letter)\n",
        "    else:\n",
        "        return 0  # Default case for unsupported characters\n",
        "\n",
        "\n",
        "# read typing_intervals.csv\n",
        "import csv\n",
        "\n",
        "# 4) Example usage:\n",
        "#    Suppose the user hits button 3 at time=0.57s with velocity=90:\n",
        "# k1, h, p = step(b_i=3, t_i=0.57, v_i=90, k_prev=k_prev, h=h)\n",
        "\n",
        "notes = []\n",
        "\n",
        "filename = \"typing_wpm_timestamps.csv\"\n",
        "filename_no_ext = filename.split(\".\")[0]\n",
        "\n",
        "with open(filename, \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    # skip header\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        print(row)\n",
        "        letter, time, wpm = row[0], row[1], row[2]\n",
        "\n",
        "        if not letter or not time:\n",
        "            print(\"Skipping empty row\")\n",
        "            continue\n",
        "\n",
        "        # check if wpm is numeric\n",
        "        # if not wpm.isnumeric():\n",
        "        #     print(f\"Skipping non-numeric wpm: {wpm}\")\n",
        "        #     continue\n",
        "\n",
        "        time = float(time)\n",
        "        wpm = float(wpm)\n",
        "\n",
        "        print(letter, time, wpm)\n",
        "        # convert letter to button index and velocity\n",
        "        letter = letter.lower()\n",
        "\n",
        "        # velocity is used from mapping function, mapping is based on keyboard layout\n",
        "        button = letter_to_button_26(letter)\n",
        "        velocity = int(wpm * 2)\n",
        "        # ensure velocity is in range 1-127\n",
        "        velocity = max(1, min(127, velocity))\n",
        "        k_prev, h, probs = step(b_i=button, t_i=time, v_i=velocity, k_prev=k_prev, h=h)\n",
        "        notes.append((k_prev.item(), time, velocity))\n",
        "print(notes)\n",
        "# generate the midi file\n",
        "import pretty_midi\n",
        "import time\n",
        "\n",
        "pm = pretty_midi.PrettyMIDI()\n",
        "instr = pretty_midi.Instrument(program=0)\n",
        "\n",
        "for i, (note, onset, vel) in enumerate(notes):\n",
        "    # define a duration for each note\n",
        "    if i + 1 < len(notes):\n",
        "        end = notes[i + 1][1]\n",
        "    else:\n",
        "        end = onset + 0.5\n",
        "    pm_note = pretty_midi.Note(velocity=vel, pitch=note, start=onset, end=end)\n",
        "    print(pm_note)\n",
        "    instr.notes.append(pm_note)\n",
        "\n",
        "pm.instruments.append(instr)\n",
        "filename = f\"output_{time.time()}_{filename_no_ext}.mid\"\n",
        "pm.write(filename)\n"
      ],
      "metadata": {
        "id": "TaIzai7w0T-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sr6Gl8Yk0sH-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}